{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4ecb04c",
   "metadata": {},
   "source": [
    "# Group 6 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974b9060",
   "metadata": {},
   "source": [
    "* Members: Michał Butkiewicz, ChiaYu Lin, Daumantas Patapas, Marcel Stelte\n",
    "* Institution: Vrije Universiteit Amserdam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2302fd29",
   "metadata": {},
   "source": [
    "This notebook will present and discuss the code used to retrieve data from the social media platforms Twitter and Sina Weibo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sina Weibo Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the crawler for the Sina Weibo social media platform will be described.\n",
    "\n",
    "The platform is most prominently used in China, therefore the searched keywords are in Chinese too. With only a Dutch phone number it is impossible to create a new account on Sina Weibo, which already prevents the usage of the API provided by Sina Weibo itself. Also if the website is accessed without an account the user will automatically be redirected to the front page. The only exception is the access through the mobile version of the website: https://m.weibo.cn. This version of the website has no restrictions regarding accounts or geolocation, and can therefore be used as a basis for the crawler.\n",
    "\n",
    "Through research an already existing crawler was found: https://github.com/KaidiGuo/keyword_based_Sina_weibo_crawler. This crawler however was not maintained for several years and not functional anymore. It still gave insight in how to construct a working crawler for Sina Weibo using the mobile website. More specifically, this crawler accesses the API that is used as a data model for the mobile version of the platform: https://m.weibo.cn/api/.\n",
    "\n",
    "For the functionality of the crawler several utility functions are necessary, which will be explained first.\n",
    "\n",
    "To make the crawled data reusable at a later point in time it was decided that the results of the crawl should be persisted in a file, so that it could be used later on. For this it needs to be checked if a fitting directory already exists, or if it has to be created. The function for this is depicted below. It checks if a folder called \"data\" exists within the location in which this notebook will be executed, and creates such a directory if it does not exist yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef5c62f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def init_directories():\n",
    "    db_path = f\"{os.path.dirname(os.path.abspath(''))}/data/\"\n",
    "\n",
    "    if not os.path.exists(db_path):\n",
    "        os.mkdir(db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, since the keywords need to be represented in an URL string which is appended to the URL of the mobile API, certain characters need to be formatted differently. In the case of this project it was only necessary to do this for the \"#\" symbol, but the following method can easily be extended with further symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_keyword_for_url(keyword: str) -> str:\n",
    "    return keyword.replace(\"#\", \"%23\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sina Weibo mobile API returns posts in form of so-called cards. They are however not returned as a list but a tree with any number of branches on each node. To make them easier processable later on a function is provided that puts the cards into a list. There are also many cards that contain no actual posts but e.g. suggestions for users that might be interesting to the user, advertisements, etc., which are not relevant for this research and are therefore filtered from the list too.\n",
    "\n",
    "The relevant numerical card_types represent the following cards:\n",
    "* mblog: Contains a singuler post that can be added to the list\n",
    "* card_group: Contains a list of cards that need to be analysed recursively\n",
    "* left_- and right_element: Some cards are split horizontally and contain two \"mblog\" cards that can be added to the list\n",
    "\n",
    "The following URL gives an example of how the posts are formatted in the API: https://m.weibo.cn/api/container/getIndex?containerid=100103type%3D1%26q%3D%23%E8%8B%B1%E9%9B%84%E8%81%94%E7%9B%9F%23%23EDG%23. The cards 0 and 1 contain a card with a singular \"mblog\", which can be added to the list. Card 2 contains a \"card_group\", which in turn contains a card with a \"mblog\" and a card without a group or a post. Therefore only the first child of card 2 should be added to the list.\n",
    "\n",
    "There is also a mechanism in place that stores the ID of each added card in a set. For each new card it will be checked whether that ID is already in the set or not. This mechanism prevents duplicate entries in the list of cards that could falsify the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def unpack_nested_cards(retrieved_cards: any) -> set[any]:\n",
    "\n",
    "    cards = []\n",
    "    card_ids = set()\n",
    "\n",
    "    for retrieved_card in retrieved_cards:\n",
    "\n",
    "        json_card = json.loads(json.dumps(retrieved_card))\n",
    "\n",
    "        if json_card.get(\"mblog\"):\n",
    "            card_id = retrieved_card[\"mblog\"][\"id\"]\n",
    "            if card_id not in card_ids:\n",
    "                cards.append(retrieved_card)\n",
    "                card_ids.add(card_id)\n",
    "\n",
    "        if json_card.get(\"card_group\"):\n",
    "            for card in unpack_nested_cards(retrieved_card[\"card_group\"]):\n",
    "                card_id = card[\"mblog\"][\"id\"]\n",
    "                if card_id not in card_ids:\n",
    "                    cards.append(card)\n",
    "                    card_ids.add(card_id)\n",
    "\n",
    "        if json_card.get(\"left_element\"):\n",
    "            card_id = retrieved_card[\"left_element\"][\"mblog\"][\"id\"]\n",
    "            if card_id not in card_ids:\n",
    "                cards.append(retrieved_card[\"left_element\"])\n",
    "                card_ids.add(card_id)\n",
    "\n",
    "        if json_card.get(\"right_element\"):\n",
    "            card_id = retrieved_card[\"right_element\"][\"mblog\"][\"id\"]\n",
    "            if card_id not in card_ids:\n",
    "                cards.append(retrieved_card[\"right_element\"])\n",
    "                card_ids.add(card_id)\n",
    "\n",
    "    return cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next utility functions focus in formatting datetimes. Since it is necessary to limit the retrieved data to a specific time frame, the creation date and time retrieved by the mobile API needs to be parsed into a datetime object, which can then be compared to previously specified datetime objects to evaluate if the current card is within the allowed timeframe.\n",
    "\n",
    "The previous example showed that the API returns the datetime in the following format: \"Sun Nov 07 02:17:19 +0800 2021\". Since the month is not returned as a numerical object, first a function that parses the month into a numerical value is needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_month_to_int(month: str) -> int:\n",
    "    if month == 'Jan':\n",
    "        return 1\n",
    "    if month == 'Feb':\n",
    "        return 2\n",
    "    if month == 'Mar':\n",
    "        return 3\n",
    "    if month == 'Apr':\n",
    "        return 4\n",
    "    if month == 'May':\n",
    "        return 5\n",
    "    if month == 'Jun':\n",
    "        return 6\n",
    "    if month == 'Jul':\n",
    "        return 7\n",
    "    if month == 'Aug':\n",
    "        return 8\n",
    "    if month == 'Sep':\n",
    "        return 9\n",
    "    if month == 'Oct':\n",
    "        return 10\n",
    "    if month == 'Nov':\n",
    "        return 11\n",
    "    if month == 'Dec':\n",
    "        return 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards the rest of the retrieved datetime string needs to be processed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def parse_creation_time_of_card(creation_time: str) -> datetime:\n",
    "    (weekday, month, day, time, timezone, year) = creation_time.split(\" \")\n",
    "    (hour, minute, second) = time.split(\":\")\n",
    "    if timezone[0:1] == \"+\":\n",
    "        actual_timezone = datetime.timezone(datetime.timedelta(hours = int(timezone[1:]) / 100))\n",
    "    elif timezone[0:1] == \"-\":\n",
    "        actual_timezone = datetime.timezone(datetime.timedelta(hours = int(timezone[1:]) / -100))\n",
    "    actual_creation_time = datetime.datetime(int(year), parse_month_to_int(month), int(day), int(hour), int(minute), int(second), 0, actual_timezone)\n",
    "\n",
    "    return actual_creation_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all those utility functions defined we can now combine the into a singular function that initiates a crawl of a single keyword. The function will first define the url and the header with the user agent that has to be passed to that url. There is also a page counter in place, since the API returns the cards split over multiple pages. After each run it will be checked if the lates page contained any cards, if not the function will end looping through them. For each page an HTML request is sent to the API, and the returned card tree will be parsed and filtered with the previously mentioned methods. Finally all cards will be stored in a text file in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def crawl_weibo(keyword: str, start_date: datetime, end_data: datetime, file_name: str):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'\n",
    "    }\n",
    "    url = \"https://m.weibo.cn/api/container/getIndex?\"\n",
    "    \n",
    "    cur_page = 0\n",
    "    data_retrieved = True\n",
    "\n",
    "    init_directories()\n",
    "\n",
    "    while data_retrieved:\n",
    "        cur_page += 1\n",
    "\n",
    "        data = {\n",
    "            'containerid': f'100103type%3D1%26q%3D{format_keyword_for_url(keyword)}',\n",
    "            'page_type':' searchall',\n",
    "            'page': cur_page,\n",
    "        }\n",
    "        html = requests.get(url, headers=headers, params=data)\n",
    "\n",
    "        if html.content:\n",
    "            response = html.json()\n",
    "            cards = response[\"data\"][\"cards\"]\n",
    "            if str(cards) != \"[]\":\n",
    "                try:\n",
    "                    for card in unpack_nested_cards(cards):\n",
    "                        creation_time = parse_creation_time_of_card(card[\"mblog\"][\"created_at\"])\n",
    "                        if creation_time >= start_date and creation_time <= end_data:\n",
    "                            with open(f\"{os.path.dirname(os.path.abspath(''))}/data/{file_name}.txt\", \"a\", encoding=\"utf-8\") as file:\n",
    "                                file.write(str(card))\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            else:\n",
    "                data_retrieved = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to execute all this list of keywords and output file names is set up together with a start and end date. For each element of the keyword list the crawl_method will be called, and the results for the given timeframe will be stored in a file with the given name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.datetime(2021, 9, 1, 0, 0, 0, 0, datetime.timezone(datetime.timedelta()))\n",
    "end_date = datetime.datetime(2021, 12, 18, 23, 59, 59, 59, datetime.timezone(datetime.timedelta()))\n",
    "\n",
    "searches = []\n",
    "searches.append([\"#PGC2021#\", \"PGC2021\"])\n",
    "searches.append([\"#斯德哥尔摩major#\", \"PGLMAJOR\"])\n",
    "searches.append([\"#2021英雄联盟全球总决赛#\", \"Worlds2021\"])\n",
    "searches.append([\"#短池世锦赛#\", \"FINAAbuDhabi2021\"])\n",
    "searches.append([\"#2021MLB世界大赛#\", \"WorldSeries\"])\n",
    "searches.append([\"#F1阿布扎比大奖赛#\", \"AbuDhabiGP\"])\n",
    "\n",
    "for search in searches:\n",
    "    crawl_weibo(search[0], start_date, end_date, search[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation & Plotting"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
